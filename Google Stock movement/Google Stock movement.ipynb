{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9948d5b4-c273-4850-bccc-855cbafb4d8e",
   "metadata": {},
   "source": [
    "# Author of project: Akinmade Faruq\n",
    "# Contact informations: www.linkedin.com/in/faruqakinmade\n",
    "# Email: Fharuk147@gmail.com\n",
    "# X website: https://x.com/EngrrrAkinmade\n",
    "\n",
    "\n",
    "# Stock Movement Prediction for Alphabet Inc. (GOOGL)\n",
    "\n",
    "## **Project Overview**\n",
    "\n",
    "This project aims to develop a robust machine learning system capable of predicting daily stock movements of Alphabet Inc. (GOOGL) by leveraging both **historical stock market data** and **public sentiment extracted from news articles**. The system integrates techniques from **Natural Language Processing (NLP)**, **time-series analysis**, and **financial modeling** to provide actionable insights for trading strategies. \n",
    "\n",
    "The predictions generated by this system are designed to be **accurate, interpretable, and compliant with regulatory standards**, serving as a benchmark for industry applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **Datasets**\n",
    "\n",
    "1. **News Articles Dataset**\n",
    "   - Contains daily news headlines and summaries about Alphabet Inc. from January 1, 2000.\n",
    "   - Key columns: `category`, `datetime`, `headline`, `summary`, `image`, `source`, `related`, `url`.\n",
    "   - Purpose: Extract sentiment and event information to understand market reactions.\n",
    "\n",
    "2. **Stock Market Dataset**\n",
    "   - Historical daily stock prices for GOOGL, including high, low, open, close, and volume.\n",
    "   - Key columns: `Price High`, `Price Low`, `Open`, `Close`, `Volume`.\n",
    "   - Purpose: Capture historical price movements and volatility for prediction modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## **Project Workflow**\n",
    "\n",
    "1. **Data Preprocessing**\n",
    "   - Clean and normalize stock and news data.\n",
    "   - Handle missing values, outliers, and duplicate records.\n",
    "   - Standardize date formats and align news with trading days.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**\n",
    "   - Visualize stock price trends, returns, and volatility.\n",
    "   - Analyze sentiment trends and correlate with stock movements.\n",
    "   - Identify patterns, anomalies, and potential event-driven effects.\n",
    "\n",
    "3. **Feature Engineering**\n",
    "   - Generate sentiment scores, topic distributions, and event indicators from news.\n",
    "   - Compute technical indicators and lagged features from stock data.\n",
    "   - Combine datasets to create enriched features for modeling.\n",
    "\n",
    "4. **Model Selection**\n",
    "   - Evaluate machine learning algorithms such as Logistic Regression, Random Forest, XGBoost, LSTM, and hybrid models.\n",
    "   - Balance predictive performance with interpretability and regulatory compliance.\n",
    "\n",
    "5. **Training and Validation**\n",
    "   - Train models using time-series aware splits and walk-forward cross-validation.\n",
    "   - Evaluate performance using accuracy, precision, recall, F1-score, and financial metrics.\n",
    "\n",
    "6. **Explanation and Interpretability**\n",
    "   - Apply SHAP and LIME for local and global interpretability.\n",
    "   - Document feature contributions and model decisions for transparency.\n",
    "\n",
    "7. **Deployment**\n",
    "   - Deploy as an API service for real-time prediction.\n",
    "   - Monitor performance, detect model drift, and periodically update the model.\n",
    "   - Ensure compliance through logging and audit trails.\n",
    "\n",
    "---\n",
    "\n",
    "By following this workflow, the system will provide **accurate predictions** for stock movements while maintaining **transparency, interpretability, and reliability**, ultimately enabling informed investment decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e7f10c-5c0f-40c7-a04e-b9fe7c141508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\AKINMADE\n",
      "[nltk_data]     FARUQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\AKINMADE\n",
      "[nltk_data]     FARUQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\AKINMADE\n",
      "[nltk_data]     FARUQ\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing Necessary Libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Natural Language Processing (NLP)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Deep Learning (if using LSTM/GRU)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Model interpretability\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fbc5351-4627-4a3c-aef4-b2acccb3a14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dataset Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>2.502503</td>\n",
       "      <td>2.604104</td>\n",
       "      <td>2.401401</td>\n",
       "      <td>2.511011</td>\n",
       "      <td>893181924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-20</td>\n",
       "      <td>2.527778</td>\n",
       "      <td>2.729730</td>\n",
       "      <td>2.515015</td>\n",
       "      <td>2.710460</td>\n",
       "      <td>456686856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-23</td>\n",
       "      <td>2.771522</td>\n",
       "      <td>2.839840</td>\n",
       "      <td>2.728979</td>\n",
       "      <td>2.737738</td>\n",
       "      <td>365122512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-24</td>\n",
       "      <td>2.783784</td>\n",
       "      <td>2.792793</td>\n",
       "      <td>2.591842</td>\n",
       "      <td>2.624374</td>\n",
       "      <td>304946748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>2.626627</td>\n",
       "      <td>2.702703</td>\n",
       "      <td>2.599600</td>\n",
       "      <td>2.652653</td>\n",
       "      <td>183772044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close     Volume\n",
       "0  2004-08-19  2.502503  2.604104  2.401401  2.511011  893181924\n",
       "1  2004-08-20  2.527778  2.729730  2.515015  2.710460  456686856\n",
       "2  2004-08-23  2.771522  2.839840  2.728979  2.737738  365122512\n",
       "3  2004-08-24  2.783784  2.792793  2.591842  2.624374  304946748\n",
       "4  2004-08-25  2.626627  2.702703  2.599600  2.652653  183772044"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News Dataset Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>datetime</th>\n",
       "      <th>headline</th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>related</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>company</td>\n",
       "      <td>1745449200</td>\n",
       "      <td>Alphabet earnings, Fed comments, Nintendo Swit...</td>\n",
       "      <td>134059226</td>\n",
       "      <td>https://s.yimg.com/rz/stage/p/yahoo_finance_en...</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>Here's what investors are watching on Thursday...</td>\n",
       "      <td>https://finnhub.io/api/news?id=5381fda0f641074...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>company</td>\n",
       "      <td>1745446095</td>\n",
       "      <td>Is Alphabet Inc. (GOOGL) the Best Stock to Buy...</td>\n",
       "      <td>134059227</td>\n",
       "      <td>https://s.yimg.com/rz/stage/p/yahoo_finance_en...</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>We recently published a list of 10 Best Stocks...</td>\n",
       "      <td>https://finnhub.io/api/news?id=bdc5b5103ae73db...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>company</td>\n",
       "      <td>1745442355</td>\n",
       "      <td>Is Alphabet Inc. (GOOG) the Best Stock to Buy ...</td>\n",
       "      <td>134059228</td>\n",
       "      <td>https://s.yimg.com/rz/stage/p/yahoo_finance_en...</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>We recently published a list of 20 Best Stocks...</td>\n",
       "      <td>https://finnhub.io/api/news?id=8cdf3969c1ec9e3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>company</td>\n",
       "      <td>1745440328</td>\n",
       "      <td>Google earnings are coming today. Here's what ...</td>\n",
       "      <td>134059229</td>\n",
       "      <td>https://s.yimg.com/rz/stage/p/yahoo_finance_en...</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>Google (GOOGL) will report first-quarter 2025 ...</td>\n",
       "      <td>https://finnhub.io/api/news?id=ed468a233b607bd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>company</td>\n",
       "      <td>1745439372</td>\n",
       "      <td>Equity Markets Close Higher Over Potential Red...</td>\n",
       "      <td>134059230</td>\n",
       "      <td>https://s.yimg.com/rz/stage/p/yahoo_finance_en...</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>US benchmark equity indexes closed higher on W...</td>\n",
       "      <td>https://finnhub.io/api/news?id=54bdad840d13d87...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category    datetime                                           headline  \\\n",
       "0  company  1745449200  Alphabet earnings, Fed comments, Nintendo Swit...   \n",
       "1  company  1745446095  Is Alphabet Inc. (GOOGL) the Best Stock to Buy...   \n",
       "2  company  1745442355  Is Alphabet Inc. (GOOG) the Best Stock to Buy ...   \n",
       "3  company  1745440328  Google earnings are coming today. Here's what ...   \n",
       "4  company  1745439372  Equity Markets Close Higher Over Potential Red...   \n",
       "\n",
       "          id                                              image related  \\\n",
       "0  134059226  https://s.yimg.com/rz/stage/p/yahoo_finance_en...   GOOGL   \n",
       "1  134059227  https://s.yimg.com/rz/stage/p/yahoo_finance_en...   GOOGL   \n",
       "2  134059228  https://s.yimg.com/rz/stage/p/yahoo_finance_en...   GOOGL   \n",
       "3  134059229  https://s.yimg.com/rz/stage/p/yahoo_finance_en...   GOOGL   \n",
       "4  134059230  https://s.yimg.com/rz/stage/p/yahoo_finance_en...   GOOGL   \n",
       "\n",
       "  source                                            summary  \\\n",
       "0  Yahoo  Here's what investors are watching on Thursday...   \n",
       "1  Yahoo  We recently published a list of 10 Best Stocks...   \n",
       "2  Yahoo  We recently published a list of 20 Best Stocks...   \n",
       "3  Yahoo  Google (GOOGL) will report first-quarter 2025 ...   \n",
       "4  Yahoo  US benchmark equity indexes closed higher on W...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://finnhub.io/api/news?id=5381fda0f641074...  \n",
       "1  https://finnhub.io/api/news?id=bdc5b5103ae73db...  \n",
       "2  https://finnhub.io/api/news?id=8cdf3969c1ec9e3...  \n",
       "3  https://finnhub.io/api/news?id=ed468a233b607bd...  \n",
       "4  https://finnhub.io/api/news?id=54bdad840d13d87...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stock Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4717 entries, 0 to 4716\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    4717 non-null   object \n",
      " 1   Open    4717 non-null   float64\n",
      " 2   High    4717 non-null   float64\n",
      " 3   Low     4717 non-null   float64\n",
      " 4   Close   4717 non-null   float64\n",
      " 5   Volume  4717 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 221.2+ KB\n",
      "None\n",
      "\n",
      "Missing values in Stock Dataset:\n",
      "Date      0\n",
      "Open      0\n",
      "High      0\n",
      "Low       0\n",
      "Close     0\n",
      "Volume    0\n",
      "dtype: int64\n",
      "\n",
      "News Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 231 entries, 0 to 230\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  231 non-null    object\n",
      " 1   datetime  231 non-null    int64 \n",
      " 2   headline  231 non-null    object\n",
      " 3   id        231 non-null    int64 \n",
      " 4   image     158 non-null    object\n",
      " 5   related   231 non-null    object\n",
      " 6   source    231 non-null    object\n",
      " 7   summary   190 non-null    object\n",
      " 8   url       231 non-null    object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 16.4+ KB\n",
      "None\n",
      "\n",
      "Missing values in News Dataset:\n",
      "category     0\n",
      "datetime     0\n",
      "headline     0\n",
      "id           0\n",
      "image       73\n",
      "related      0\n",
      "source       0\n",
      "summary     41\n",
      "url          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the Datasets\n",
    "\n",
    "# File paths\n",
    "stock_file = r\"C:\\Users\\AKINMADE FARUQ\\Downloads\\PROJECT MATERIALS\\My Projects\\GITHUB\\Google Stocks Prediction Analysis\\GOOGLE.csv\"\n",
    "news_file  = r\"C:\\Users\\AKINMADE FARUQ\\Downloads\\PROJECT MATERIALS\\My Projects\\GITHUB\\Google Stocks Prediction Analysis\\Google_Daily_News.csv\"\n",
    "\n",
    "# Load datasets\n",
    "stock_df = pd.read_csv(stock_file)\n",
    "news_df  = pd.read_csv(news_file)\n",
    "\n",
    "# Display first few rows of each dataset\n",
    "print(\"Stock Dataset Preview:\")\n",
    "display(stock_df.head())\n",
    "\n",
    "print(\"\\nNews Dataset Preview:\")\n",
    "display(news_df.head())\n",
    "\n",
    "# Check basic info and missing values\n",
    "print(\"\\nStock Dataset Info:\")\n",
    "print(stock_df.info())\n",
    "print(\"\\nMissing values in Stock Dataset:\")\n",
    "print(stock_df.isnull().sum())\n",
    "\n",
    "print(\"\\nNews Dataset Info:\")\n",
    "print(news_df.info())\n",
    "print(\"\\nMissing values in News Dataset:\")\n",
    "print(news_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce2e9bb-5563-407a-88d3-b6b8dc95f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Stock Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>2.502503</td>\n",
       "      <td>2.604104</td>\n",
       "      <td>2.401401</td>\n",
       "      <td>2.511011</td>\n",
       "      <td>893181924</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-20</td>\n",
       "      <td>2.527778</td>\n",
       "      <td>2.729730</td>\n",
       "      <td>2.515015</td>\n",
       "      <td>2.710460</td>\n",
       "      <td>456686856</td>\n",
       "      <td>0.079430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-23</td>\n",
       "      <td>2.771522</td>\n",
       "      <td>2.839840</td>\n",
       "      <td>2.728979</td>\n",
       "      <td>2.737738</td>\n",
       "      <td>365122512</td>\n",
       "      <td>0.010064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-24</td>\n",
       "      <td>2.783784</td>\n",
       "      <td>2.792793</td>\n",
       "      <td>2.591842</td>\n",
       "      <td>2.624374</td>\n",
       "      <td>304946748</td>\n",
       "      <td>-0.041408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>2.626627</td>\n",
       "      <td>2.702703</td>\n",
       "      <td>2.599600</td>\n",
       "      <td>2.652653</td>\n",
       "      <td>183772044</td>\n",
       "      <td>0.010776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date      Open      High       Low     Close     Volume    Return\n",
       "0 2004-08-19  2.502503  2.604104  2.401401  2.511011  893181924       NaN\n",
       "1 2004-08-20  2.527778  2.729730  2.515015  2.710460  456686856  0.079430\n",
       "2 2004-08-23  2.771522  2.839840  2.728979  2.737738  365122512  0.010064\n",
       "3 2004-08-24  2.783784  2.792793  2.591842  2.624374  304946748 -0.041408\n",
       "4 2004-08-25  2.626627  2.702703  2.599600  2.652653  183772044  0.010776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed News Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>headline</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-16 15:21:00</td>\n",
       "      <td>Big Tech’s China Risks Go Far Beyond Nvidia</td>\n",
       "      <td>Big Tech’s China Risks Go Far Beyond Nvidia</td>\n",
       "      <td>Big Tech’s China Risks Go Far Beyond Nvidia. B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-16 17:00:24</td>\n",
       "      <td>Is Alphabet Inc. (NASDAQ:GOOGL) the Best Machi...</td>\n",
       "      <td>We recently published a list of the 10 Best Ma...</td>\n",
       "      <td>Is Alphabet Inc. (NASDAQ:GOOGL) the Best Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-16 17:19:59</td>\n",
       "      <td>Prominent Investor Unloads His GOOG Stock</td>\n",
       "      <td>Well-known investor Josh Brown disclosed on CN...</td>\n",
       "      <td>Prominent Investor Unloads His GOOG Stock. Wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-16 17:37:07</td>\n",
       "      <td>Communications Services Slide on Flight From R...</td>\n",
       "      <td>Communications-services companies slid as trad...</td>\n",
       "      <td>Communications Services Slide on Flight From R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-16 18:39:10</td>\n",
       "      <td>Temu, Shein slash digital ads as tariffs end c...</td>\n",
       "      <td>Chinese online marketplace Temu and fast-fashi...</td>\n",
       "      <td>Temu, Shein slash digital ads as tariffs end c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime                                           headline  \\\n",
       "0 2025-04-16 15:21:00        Big Tech’s China Risks Go Far Beyond Nvidia   \n",
       "1 2025-04-16 17:00:24  Is Alphabet Inc. (NASDAQ:GOOGL) the Best Machi...   \n",
       "2 2025-04-16 17:19:59          Prominent Investor Unloads His GOOG Stock   \n",
       "3 2025-04-16 17:37:07  Communications Services Slide on Flight From R...   \n",
       "4 2025-04-16 18:39:10  Temu, Shein slash digital ads as tariffs end c...   \n",
       "\n",
       "                                             summary  \\\n",
       "0        Big Tech’s China Risks Go Far Beyond Nvidia   \n",
       "1  We recently published a list of the 10 Best Ma...   \n",
       "2  Well-known investor Josh Brown disclosed on CN...   \n",
       "3  Communications-services companies slid as trad...   \n",
       "4  Chinese online marketplace Temu and fast-fashi...   \n",
       "\n",
       "                                                text  \n",
       "0  Big Tech’s China Risks Go Far Beyond Nvidia. B...  \n",
       "1  Is Alphabet Inc. (NASDAQ:GOOGL) the Best Machi...  \n",
       "2  Prominent Investor Unloads His GOOG Stock. Wel...  \n",
       "3  Communications Services Slide on Flight From R...  \n",
       "4  Temu, Shein slash digital ads as tariffs end c...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# ---------- Stock Dataset ----------\n",
    "# Convert 'Date' to datetime format\n",
    "stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "\n",
    "# Sort by date\n",
    "stock_df = stock_df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Create 'Daily Return' column\n",
    "stock_df['Return'] = stock_df['Close'].pct_change()\n",
    "\n",
    "# ---------- News Dataset ----------\n",
    "# Convert 'datetime' from Unix timestamp to datetime\n",
    "news_df['datetime'] = pd.to_datetime(news_df['datetime'], unit='s')\n",
    "\n",
    "# Handle missing summaries by filling with empty string\n",
    "news_df['summary'] = news_df['summary'].fillna('')\n",
    "\n",
    "# Optional: fill missing images with placeholder\n",
    "news_df['image'] = news_df['image'].fillna('No Image')\n",
    "\n",
    "# Combine headline and summary into a single text column for NLP\n",
    "news_df['text'] = news_df['headline'] + '. ' + news_df['summary']\n",
    "\n",
    "# Sort news by datetime\n",
    "news_df = news_df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Preview processed datasets\n",
    "print(\"Processed Stock Dataset:\")\n",
    "display(stock_df.head())\n",
    "\n",
    "print(\"\\nProcessed News Dataset:\")\n",
    "display(news_df[['datetime', 'headline', 'summary', 'text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b35e5be8-7206-4b7b-b1ae-b7f6a12756a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Aggregated Sentiment:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_only</th>\n",
       "      <th>polarity_mean</th>\n",
       "      <th>polarity_max</th>\n",
       "      <th>polarity_min</th>\n",
       "      <th>polarity_std</th>\n",
       "      <th>subjectivity_mean</th>\n",
       "      <th>subjectivity_max</th>\n",
       "      <th>subjectivity_min</th>\n",
       "      <th>subjectivity_std</th>\n",
       "      <th>headline_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-16</td>\n",
       "      <td>0.155092</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>-0.027778</td>\n",
       "      <td>0.142606</td>\n",
       "      <td>0.465332</td>\n",
       "      <td>0.571493</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.107247</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-17</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.239106</td>\n",
       "      <td>0.373574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241037</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-18</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>-0.275000</td>\n",
       "      <td>0.149829</td>\n",
       "      <td>0.408563</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.178624</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-19</td>\n",
       "      <td>0.034643</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>0.358221</td>\n",
       "      <td>0.552976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361986</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-20</td>\n",
       "      <td>0.119365</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193224</td>\n",
       "      <td>0.270678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339756</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_only  polarity_mean  polarity_max  polarity_min  polarity_std  \\\n",
       "0 2025-04-16       0.155092      0.325000     -0.027778      0.142606   \n",
       "1 2025-04-17       0.003625      0.500000     -0.500000      0.239106   \n",
       "2 2025-04-18       0.085505      0.316667     -0.275000      0.149829   \n",
       "3 2025-04-19       0.034643      0.500000     -0.312500      0.358221   \n",
       "4 2025-04-20       0.119365      0.500000      0.000000      0.193224   \n",
       "\n",
       "   subjectivity_mean  subjectivity_max  subjectivity_min  subjectivity_std  \\\n",
       "0           0.465332          0.571493          0.277778          0.107247   \n",
       "1           0.373574          1.000000          0.000000          0.241037   \n",
       "2           0.408563          0.833333          0.050000          0.178624   \n",
       "3           0.552976          1.000000          0.000000          0.361986   \n",
       "4           0.270678          1.000000          0.000000          0.339756   \n",
       "\n",
       "   headline_count  \n",
       "0               7  \n",
       "1              44  \n",
       "2              28  \n",
       "3               5  \n",
       "4              11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sentiment Analysis on News\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Function to calculate polarity and subjectivity\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return pd.Series([blob.sentiment.polarity, blob.sentiment.subjectivity])\n",
    "\n",
    "# Apply sentiment analysis\n",
    "news_df[['polarity', 'subjectivity']] = news_df['text'].apply(get_sentiment)\n",
    "\n",
    "# Aggregate sentiment by date\n",
    "news_df['date_only'] = news_df['datetime'].dt.date\n",
    "daily_sentiment = news_df.groupby('date_only').agg({\n",
    "    'polarity': ['mean', 'max', 'min', 'std'],\n",
    "    'subjectivity': ['mean', 'max', 'min', 'std'],\n",
    "    'headline': 'count'  # number of news articles per day\n",
    "})\n",
    "\n",
    "# Flatten multi-level columns\n",
    "daily_sentiment.columns = ['_'.join(col) for col in daily_sentiment.columns]\n",
    "daily_sentiment = daily_sentiment.reset_index()\n",
    "daily_sentiment['date_only'] = pd.to_datetime(daily_sentiment['date_only'])\n",
    "\n",
    "# Preview daily sentiment\n",
    "print(\"Daily Aggregated Sentiment:\")\n",
    "display(daily_sentiment.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78fe1f18-9800-47bb-8716-e8d7580a9bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Stock + Sentiment Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Return</th>\n",
       "      <th>date_only</th>\n",
       "      <th>polarity_mean</th>\n",
       "      <th>polarity_max</th>\n",
       "      <th>polarity_min</th>\n",
       "      <th>polarity_std</th>\n",
       "      <th>subjectivity_mean</th>\n",
       "      <th>subjectivity_max</th>\n",
       "      <th>subjectivity_min</th>\n",
       "      <th>subjectivity_std</th>\n",
       "      <th>headline_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>2.502503</td>\n",
       "      <td>2.604104</td>\n",
       "      <td>2.401401</td>\n",
       "      <td>2.511011</td>\n",
       "      <td>893181924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-20</td>\n",
       "      <td>2.527778</td>\n",
       "      <td>2.729730</td>\n",
       "      <td>2.515015</td>\n",
       "      <td>2.710460</td>\n",
       "      <td>456686856</td>\n",
       "      <td>0.079430</td>\n",
       "      <td>2004-08-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-23</td>\n",
       "      <td>2.771522</td>\n",
       "      <td>2.839840</td>\n",
       "      <td>2.728979</td>\n",
       "      <td>2.737738</td>\n",
       "      <td>365122512</td>\n",
       "      <td>0.010064</td>\n",
       "      <td>2004-08-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-24</td>\n",
       "      <td>2.783784</td>\n",
       "      <td>2.792793</td>\n",
       "      <td>2.591842</td>\n",
       "      <td>2.624374</td>\n",
       "      <td>304946748</td>\n",
       "      <td>-0.041408</td>\n",
       "      <td>2004-08-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>2.626627</td>\n",
       "      <td>2.702703</td>\n",
       "      <td>2.599600</td>\n",
       "      <td>2.652653</td>\n",
       "      <td>183772044</td>\n",
       "      <td>0.010776</td>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date      Open      High       Low     Close     Volume    Return  \\\n",
       "0 2004-08-19  2.502503  2.604104  2.401401  2.511011  893181924       NaN   \n",
       "1 2004-08-20  2.527778  2.729730  2.515015  2.710460  456686856  0.079430   \n",
       "2 2004-08-23  2.771522  2.839840  2.728979  2.737738  365122512  0.010064   \n",
       "3 2004-08-24  2.783784  2.792793  2.591842  2.624374  304946748 -0.041408   \n",
       "4 2004-08-25  2.626627  2.702703  2.599600  2.652653  183772044  0.010776   \n",
       "\n",
       "   date_only  polarity_mean  polarity_max  polarity_min  polarity_std  \\\n",
       "0 2004-08-19            0.0           0.0           0.0           0.0   \n",
       "1 2004-08-20            0.0           0.0           0.0           0.0   \n",
       "2 2004-08-23            0.0           0.0           0.0           0.0   \n",
       "3 2004-08-24            0.0           0.0           0.0           0.0   \n",
       "4 2004-08-25            0.0           0.0           0.0           0.0   \n",
       "\n",
       "   subjectivity_mean  subjectivity_max  subjectivity_min  subjectivity_std  \\\n",
       "0                0.0               0.0               0.0               0.0   \n",
       "1                0.0               0.0               0.0               0.0   \n",
       "2                0.0               0.0               0.0               0.0   \n",
       "3                0.0               0.0               0.0               0.0   \n",
       "4                0.0               0.0               0.0               0.0   \n",
       "\n",
       "   headline_count  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge Stock Data with Sentiment Features\n",
    "\n",
    "# Merge stock_df and daily_sentiment on date\n",
    "stock_df['date_only'] = stock_df['Date'].dt.date\n",
    "stock_df['date_only'] = pd.to_datetime(stock_df['date_only'])\n",
    "\n",
    "# Merge with sentiment data\n",
    "combined_df = pd.merge(stock_df, daily_sentiment, on='date_only', how='left')\n",
    "\n",
    "# Fill missing sentiment values with 0 (no news days)\n",
    "sentiment_cols = [col for col in combined_df.columns if 'polarity' in col or 'subjectivity' in col or 'headline_count' in col]\n",
    "combined_df[sentiment_cols] = combined_df[sentiment_cols].fillna(0)\n",
    "\n",
    "# Preview the combined dataset\n",
    "print(\"Combined Stock + Sentiment Dataset:\")\n",
    "display(combined_df.head())\n",
    "\n",
    "# Drop temporary 'date_only' if not needed\n",
    "# combined_df = combined_df.drop(columns=['date_only'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a9cdf41-cbda-4fc6-8904-8fd7b2ed1c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineered Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Return</th>\n",
       "      <th>date_only</th>\n",
       "      <th>polarity_mean</th>\n",
       "      <th>polarity_max</th>\n",
       "      <th>...</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Volatility_5</th>\n",
       "      <th>Volatility_10</th>\n",
       "      <th>Return_1d</th>\n",
       "      <th>Return_2d</th>\n",
       "      <th>Return_3d</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-09-16</td>\n",
       "      <td>2.811311</td>\n",
       "      <td>2.897898</td>\n",
       "      <td>2.794044</td>\n",
       "      <td>2.852102</td>\n",
       "      <td>185326488</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>2004-09-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.647648</td>\n",
       "      <td>2.634647</td>\n",
       "      <td>2.696439</td>\n",
       "      <td>2.641854</td>\n",
       "      <td>0.012349</td>\n",
       "      <td>0.014850</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.037116</td>\n",
       "      <td>0.020602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-09-17</td>\n",
       "      <td>2.863363</td>\n",
       "      <td>2.940190</td>\n",
       "      <td>2.841592</td>\n",
       "      <td>2.940190</td>\n",
       "      <td>189450360</td>\n",
       "      <td>0.030885</td>\n",
       "      <td>2004-09-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.687638</td>\n",
       "      <td>2.656106</td>\n",
       "      <td>2.740757</td>\n",
       "      <td>2.670267</td>\n",
       "      <td>0.012574</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.037116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-09-20</td>\n",
       "      <td>2.926677</td>\n",
       "      <td>3.043043</td>\n",
       "      <td>2.922172</td>\n",
       "      <td>2.986987</td>\n",
       "      <td>212575212</td>\n",
       "      <td>0.015916</td>\n",
       "      <td>2004-09-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.736061</td>\n",
       "      <td>2.669932</td>\n",
       "      <td>2.785526</td>\n",
       "      <td>2.700431</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.030885</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-09-21</td>\n",
       "      <td>2.998248</td>\n",
       "      <td>3.013514</td>\n",
       "      <td>2.940691</td>\n",
       "      <td>2.948949</td>\n",
       "      <td>144575280</td>\n",
       "      <td>-0.012735</td>\n",
       "      <td>2004-09-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.776752</td>\n",
       "      <td>2.680493</td>\n",
       "      <td>2.815240</td>\n",
       "      <td>2.724100</td>\n",
       "      <td>0.016335</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.015916</td>\n",
       "      <td>0.030885</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-09-22</td>\n",
       "      <td>2.937938</td>\n",
       "      <td>2.994745</td>\n",
       "      <td>2.923173</td>\n",
       "      <td>2.962462</td>\n",
       "      <td>151624224</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>2004-09-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.816992</td>\n",
       "      <td>2.697397</td>\n",
       "      <td>2.842007</td>\n",
       "      <td>2.746801</td>\n",
       "      <td>0.016334</td>\n",
       "      <td>0.015632</td>\n",
       "      <td>-0.012735</td>\n",
       "      <td>0.015916</td>\n",
       "      <td>0.030885</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date      Open      High       Low     Close     Volume    Return  \\\n",
       "0 2004-09-16  2.811311  2.897898  2.794044  2.852102  185326488  0.017589   \n",
       "1 2004-09-17  2.863363  2.940190  2.841592  2.940190  189450360  0.030885   \n",
       "2 2004-09-20  2.926677  3.043043  2.922172  2.986987  212575212  0.015916   \n",
       "3 2004-09-21  2.998248  3.013514  2.940691  2.948949  144575280 -0.012735   \n",
       "4 2004-09-22  2.937938  2.994745  2.923173  2.962462  151624224  0.004582   \n",
       "\n",
       "   date_only  polarity_mean  polarity_max  ...    SMA_10    SMA_20    EMA_10  \\\n",
       "0 2004-09-16            0.0           0.0  ...  2.647648  2.634647  2.696439   \n",
       "1 2004-09-17            0.0           0.0  ...  2.687638  2.656106  2.740757   \n",
       "2 2004-09-20            0.0           0.0  ...  2.736061  2.669932  2.785526   \n",
       "3 2004-09-21            0.0           0.0  ...  2.776752  2.680493  2.815240   \n",
       "4 2004-09-22            0.0           0.0  ...  2.816992  2.697397  2.842007   \n",
       "\n",
       "     EMA_20  Volatility_5  Volatility_10  Return_1d  Return_2d  Return_3d  \\\n",
       "0  2.641854      0.012349       0.014850   0.004574   0.037116   0.020602   \n",
       "1  2.670267      0.012574       0.015884   0.017589   0.004574   0.037116   \n",
       "2  2.700431      0.012889       0.012021   0.030885   0.017589   0.004574   \n",
       "3  2.724100      0.016335       0.015469   0.015916   0.030885   0.017589   \n",
       "4  2.746801      0.016334       0.015632  -0.012735   0.015916   0.030885   \n",
       "\n",
       "   Target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       0  \n",
       "3       1  \n",
       "4       1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Moving Averages\n",
    "combined_df['SMA_5'] = combined_df['Close'].rolling(window=5).mean()\n",
    "combined_df['SMA_10'] = combined_df['Close'].rolling(window=10).mean()\n",
    "combined_df['SMA_20'] = combined_df['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Exponential Moving Average\n",
    "combined_df['EMA_10'] = combined_df['Close'].ewm(span=10, adjust=False).mean()\n",
    "combined_df['EMA_20'] = combined_df['Close'].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "# Rolling Volatility (Standard Deviation of Returns)\n",
    "combined_df['Volatility_5'] = combined_df['Return'].rolling(window=5).std()\n",
    "combined_df['Volatility_10'] = combined_df['Return'].rolling(window=10).std()\n",
    "\n",
    "# Lag Features (previous day returns)\n",
    "combined_df['Return_1d'] = combined_df['Return'].shift(1)\n",
    "combined_df['Return_2d'] = combined_df['Return'].shift(2)\n",
    "combined_df['Return_3d'] = combined_df['Return'].shift(3)\n",
    "\n",
    "# Target variable: Direction (Up = 1, Down = 0)\n",
    "combined_df['Target'] = np.where(combined_df['Return'].shift(-1) > 0, 1, 0)\n",
    "\n",
    "# Drop rows with NaN values created by rolling and shift\n",
    "combined_df = combined_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Preview engineered dataset\n",
    "print(\"Feature Engineered Dataset:\")\n",
    "display(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747bc03a-a3c7-4f4d-b706-f2a00d57122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing data prepared!\n",
      "X_train shape: (3758, 24), X_test shape: (940, 24)\n",
      "y_train distribution:\n",
      "Target\n",
      "1    1952\n",
      "0    1806\n",
      "Name: count, dtype: int64\n",
      "y_test distribution:\n",
      "Target\n",
      "1    497\n",
      "0    443\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split and Feature Scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define feature columns (exclude Date, Target, and date_only)\n",
    "feature_cols = [col for col in combined_df.columns if col not in ['Date', 'Target', 'date_only', 'Return']]\n",
    "\n",
    "X = combined_df[feature_cols]\n",
    "y = combined_df['Target']\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training and testing data prepared!\")\n",
    "print(f\"X_train shape: {X_train_scaled.shape}, X_test shape: {X_test_scaled.shape}\")\n",
    "print(f\"y_train distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"y_test distribution:\\n{y_test.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "833f0c43-ad0d-45ef-af31-940c29dca1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Performance:\n",
      "Accuracy : 0.4745\n",
      "Precision: 0.5484\n",
      "Recall   : 0.0342\n",
      "F1-Score : 0.0644\n",
      "\n",
      "Confusion Matrix:\n",
      "[[429  14]\n",
      " [480  17]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.97      0.63       443\n",
      "           1       0.55      0.03      0.06       497\n",
      "\n",
      "    accuracy                           0.47       940\n",
      "   macro avg       0.51      0.50      0.35       940\n",
      "weighted avg       0.51      0.47      0.33       940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest Model Performance:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-Score : {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b61370c-578f-4a94-a026-fbe25e1870f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training target distribution:\n",
      "Counter({1: 1952, 0: 1806})\n",
      "\n",
      "After SMOTE, training target distribution:\n",
      "Counter({1: 1952, 0: 1952})\n",
      "\n",
      "X_train_res shape: (3904, 24), y_train_res shape: (3904,)\n"
     ]
    }
   ],
   "source": [
    "# Check and Handle Class Imbalance\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Check target distribution\n",
    "print(\"Original training target distribution:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Check the new distribution\n",
    "print(\"\\nAfter SMOTE, training target distribution:\")\n",
    "print(Counter(y_train_res))\n",
    "\n",
    "# Shapes after resampling\n",
    "print(f\"\\nX_train_res shape: {X_train_res.shape}, y_train_res shape: {y_train_res.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55bcfa5f-74f4-4844-9ecb-7f12e76ac192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Performance After Balancing:\n",
      "Accuracy : 0.4777\n",
      "Precision: 0.6154\n",
      "Recall   : 0.0322\n",
      "F1-Score : 0.0612\n",
      "\n",
      "Confusion Matrix:\n",
      "[[433  10]\n",
      " [481  16]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.98      0.64       443\n",
      "           1       0.62      0.03      0.06       497\n",
      "\n",
      "    accuracy                           0.48       940\n",
      "   macro avg       0.54      0.50      0.35       940\n",
      "weighted avg       0.55      0.48      0.33       940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrain Random Forest on Balanced Data\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model_balanced = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Train the model on SMOTE-resampled data\n",
    "rf_model_balanced.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predict on the original test set\n",
    "y_pred_balanced = rf_model_balanced.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_balanced)\n",
    "precision = precision_score(y_test, y_pred_balanced)\n",
    "recall = recall_score(y_test, y_pred_balanced)\n",
    "f1 = f1_score(y_test, y_pred_balanced)\n",
    "cm = confusion_matrix(y_test, y_pred_balanced)\n",
    "\n",
    "print(\"Random Forest Model Performance After Balancing:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-Score : {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceb465b7-2e2e-44e9-a64b-c1fed9097a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences: 118\n",
      "Number of testing sequences: 30\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data for LSTM / GRU\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "\n",
    "# Define the number of time steps for sequences\n",
    "time_steps = 5  # you can adjust\n",
    "\n",
    "# Features and target\n",
    "X_lstm = combined_df[feature_cols].values\n",
    "y_lstm = combined_df['Target'].values\n",
    "\n",
    "# Scale features (already scaled in previous StandardScaler)\n",
    "X_lstm_scaled = scaler.fit_transform(X_lstm)\n",
    "\n",
    "# Split into train and test sets (chronologically)\n",
    "train_size = int(len(X_lstm_scaled) * 0.8)\n",
    "X_train_lstm, X_test_lstm = X_lstm_scaled[:train_size], X_lstm_scaled[train_size:]\n",
    "y_train_lstm, y_test_lstm = y_lstm[:train_size], y_lstm[train_size:]\n",
    "\n",
    "# Create sequences using TimeseriesGenerator\n",
    "train_generator = TimeseriesGenerator(X_train_lstm, y_train_lstm, length=time_steps, batch_size=32)\n",
    "test_generator = TimeseriesGenerator(X_test_lstm, y_test_lstm, length=time_steps, batch_size=32)\n",
    "\n",
    "print(f\"Number of training sequences: {len(train_generator)}\")\n",
    "print(f\"Number of testing sequences: {len(test_generator)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f08b7fa5-743c-45d2-9da9-177819fb0f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.5027 - loss: 0.7058 - val_accuracy: 0.5359 - val_loss: 0.6912\n",
      "Epoch 2/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5126 - loss: 0.7006 - val_accuracy: 0.5080 - val_loss: 0.6946\n",
      "Epoch 3/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5213 - loss: 0.6939 - val_accuracy: 0.5279 - val_loss: 0.6957\n",
      "Epoch 4/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5220 - loss: 0.6940 - val_accuracy: 0.5306 - val_loss: 0.6950\n",
      "Epoch 5/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5246 - loss: 0.6951 - val_accuracy: 0.5306 - val_loss: 0.6955\n",
      "Epoch 6/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5296 - loss: 0.6896 - val_accuracy: 0.5266 - val_loss: 0.6974\n",
      "Epoch 7/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5183 - loss: 0.6920 - val_accuracy: 0.5226 - val_loss: 0.6989\n",
      "Epoch 8/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5329 - loss: 0.6907 - val_accuracy: 0.5279 - val_loss: 0.6987\n",
      "Epoch 9/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5409 - loss: 0.6908 - val_accuracy: 0.5452 - val_loss: 0.6969\n",
      "Epoch 10/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5376 - loss: 0.6887 - val_accuracy: 0.5332 - val_loss: 0.7024\n",
      "Epoch 11/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5506 - loss: 0.6865 - val_accuracy: 0.5080 - val_loss: 0.7009\n",
      "Epoch 12/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5309 - loss: 0.6906 - val_accuracy: 0.5386 - val_loss: 0.6969\n",
      "Epoch 13/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5339 - loss: 0.6880 - val_accuracy: 0.5359 - val_loss: 0.7015\n",
      "Epoch 14/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5319 - loss: 0.6900 - val_accuracy: 0.5559 - val_loss: 0.6970\n",
      "Epoch 15/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5399 - loss: 0.6873 - val_accuracy: 0.5332 - val_loss: 0.6956\n",
      "Epoch 16/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5266 - loss: 0.6918 - val_accuracy: 0.5492 - val_loss: 0.6945\n",
      "Epoch 17/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5436 - loss: 0.6878 - val_accuracy: 0.5013 - val_loss: 0.7000\n",
      "Epoch 18/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5432 - loss: 0.6892 - val_accuracy: 0.4854 - val_loss: 0.7022\n",
      "Epoch 19/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5329 - loss: 0.6886 - val_accuracy: 0.5426 - val_loss: 0.6972\n",
      "Epoch 20/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5439 - loss: 0.6874 - val_accuracy: 0.5452 - val_loss: 0.6965\n",
      "Epoch 21/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5449 - loss: 0.6845 - val_accuracy: 0.5479 - val_loss: 0.6964\n",
      "Epoch 22/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5319 - loss: 0.6880 - val_accuracy: 0.5332 - val_loss: 0.6949\n",
      "Epoch 23/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5432 - loss: 0.6866 - val_accuracy: 0.5266 - val_loss: 0.6964\n",
      "Epoch 24/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5426 - loss: 0.6863 - val_accuracy: 0.5372 - val_loss: 0.6969\n",
      "Epoch 25/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5482 - loss: 0.6842 - val_accuracy: 0.5120 - val_loss: 0.7024\n",
      "Epoch 26/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5406 - loss: 0.6836 - val_accuracy: 0.5346 - val_loss: 0.7039\n",
      "Epoch 27/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5422 - loss: 0.6865 - val_accuracy: 0.5332 - val_loss: 0.7014\n",
      "Epoch 28/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5422 - loss: 0.6867 - val_accuracy: 0.4987 - val_loss: 0.7065\n",
      "Epoch 29/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5442 - loss: 0.6836 - val_accuracy: 0.5173 - val_loss: 0.7032\n",
      "Epoch 30/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5499 - loss: 0.6849 - val_accuracy: 0.5399 - val_loss: 0.7002\n",
      "Epoch 31/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5439 - loss: 0.6836 - val_accuracy: 0.4641 - val_loss: 0.7204\n",
      "Epoch 32/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5356 - loss: 0.6844 - val_accuracy: 0.4681 - val_loss: 0.7280\n",
      "Epoch 33/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5432 - loss: 0.6854 - val_accuracy: 0.4668 - val_loss: 0.7257\n",
      "Epoch 34/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5532 - loss: 0.6848 - val_accuracy: 0.4787 - val_loss: 0.7127\n",
      "Epoch 35/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5546 - loss: 0.6831 - val_accuracy: 0.5332 - val_loss: 0.7026\n",
      "Epoch 36/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5552 - loss: 0.6823 - val_accuracy: 0.5306 - val_loss: 0.6979\n",
      "Epoch 37/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5552 - loss: 0.6810 - val_accuracy: 0.5186 - val_loss: 0.7022\n",
      "Epoch 38/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5439 - loss: 0.6833 - val_accuracy: 0.4614 - val_loss: 0.7158\n",
      "Epoch 39/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5492 - loss: 0.6821 - val_accuracy: 0.4960 - val_loss: 0.7057\n",
      "Epoch 40/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5592 - loss: 0.6797 - val_accuracy: 0.5173 - val_loss: 0.7031\n",
      "Epoch 41/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5632 - loss: 0.6793 - val_accuracy: 0.4694 - val_loss: 0.7176\n",
      "Epoch 42/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5605 - loss: 0.6802 - val_accuracy: 0.5173 - val_loss: 0.7044\n",
      "Epoch 43/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5622 - loss: 0.6797 - val_accuracy: 0.4628 - val_loss: 0.7193\n",
      "Epoch 44/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5609 - loss: 0.6817 - val_accuracy: 0.4694 - val_loss: 0.7143\n",
      "Epoch 45/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5449 - loss: 0.6838 - val_accuracy: 0.4827 - val_loss: 0.7123\n",
      "Epoch 46/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5632 - loss: 0.6794 - val_accuracy: 0.4987 - val_loss: 0.7087\n",
      "Epoch 47/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5655 - loss: 0.6793 - val_accuracy: 0.5213 - val_loss: 0.7079\n",
      "Epoch 48/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5576 - loss: 0.6798 - val_accuracy: 0.4787 - val_loss: 0.7223\n",
      "Epoch 49/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5612 - loss: 0.6815 - val_accuracy: 0.4774 - val_loss: 0.7218\n",
      "Epoch 50/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5579 - loss: 0.6786 - val_accuracy: 0.4721 - val_loss: 0.7255\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step  \n",
      "MLP Model Performance:\n",
      "Accuracy : 0.4681\n",
      "Precision: 0.4545\n",
      "Recall   : 0.0302\n",
      "F1-Score : 0.0566\n",
      "\n",
      "Confusion Matrix:\n",
      "[[425  18]\n",
      " [482  15]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.96      0.63       443\n",
      "           1       0.45      0.03      0.06       497\n",
      "\n",
      "    accuracy                           0.47       940\n",
      "   macro avg       0.46      0.49      0.34       940\n",
      "weighted avg       0.46      0.47      0.33       940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a Feedforward Neural Network (MLP)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model architecture\n",
    "mlp_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(X_train_scaled, y_train_res,\n",
    "                        validation_split=0.2,\n",
    "                        epochs=50,\n",
    "                        batch_size=32,\n",
    "                        verbose=1)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_mlp = (mlp_model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_mlp)\n",
    "precision = precision_score(y_test, y_pred_mlp)\n",
    "recall = recall_score(y_test, y_pred_mlp)\n",
    "f1 = f1_score(y_test, y_pred_mlp)\n",
    "cm = confusion_matrix(y_test, y_pred_mlp)\n",
    "\n",
    "print(\"MLP Model Performance:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-Score : {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "353b473c-2560-4c26-8ec2-3422aef8a3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from mlp_tuning\\google_stock_mlp\\tuner0.json\n",
      "WARNING:tensorflow:From C:\\Users\\AKINMADE FARUQ\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Best Hyperparameters:\n",
      "{'num_layers': 2, 'units_0': 96, 'dropout_0': 0.4, 'learning_rate': 0.001, 'units_1': 32, 'dropout_1': 0.2, 'units_2': 96, 'dropout_2': 0.1, 'tuner/epochs': 10, 'tuner/initial_epoch': 4, 'tuner/bracket': 2, 'tuner/round': 1, 'tuner/trial_id': '0065'}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning with Keras Tuner\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model building function for Keras Tuner\n",
    "def build_mlp_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Tune number of layers (1-3)\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=128, step=32),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1)))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Tune learning rate\n",
    "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(build_mlp_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=30,\n",
    "                     factor=3,\n",
    "                     directory='mlp_tuning',\n",
    "                     project_name='google_stock_mlp')\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(X_train_scaled, y_train_res, validation_split=0.2, epochs=30, batch_size=32, verbose=1)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f45ccc3-915c-49b0-879d-0a8512f6a43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5000 - loss: 0.7120 - val_accuracy: 0.5160 - val_loss: 0.6909\n",
      "Epoch 2/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5273 - loss: 0.6963 - val_accuracy: 0.5253 - val_loss: 0.6939\n",
      "Epoch 3/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5196 - loss: 0.6948 - val_accuracy: 0.5199 - val_loss: 0.6936\n",
      "Epoch 4/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5110 - loss: 0.6962 - val_accuracy: 0.5279 - val_loss: 0.6982\n",
      "Epoch 5/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5173 - loss: 0.6955 - val_accuracy: 0.5226 - val_loss: 0.6947\n",
      "Epoch 6/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5133 - loss: 0.6926 - val_accuracy: 0.5279 - val_loss: 0.6984\n",
      "Epoch 7/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5263 - loss: 0.6914 - val_accuracy: 0.5332 - val_loss: 0.6963\n",
      "Epoch 8/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5246 - loss: 0.6918 - val_accuracy: 0.4973 - val_loss: 0.6983\n",
      "Epoch 9/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5266 - loss: 0.6930 - val_accuracy: 0.5306 - val_loss: 0.7007\n",
      "Epoch 10/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5240 - loss: 0.6913 - val_accuracy: 0.5053 - val_loss: 0.6971\n",
      "Epoch 11/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5210 - loss: 0.6919 - val_accuracy: 0.5106 - val_loss: 0.6957\n",
      "Epoch 12/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5250 - loss: 0.6915 - val_accuracy: 0.5293 - val_loss: 0.6951\n",
      "Epoch 13/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5359 - loss: 0.6905 - val_accuracy: 0.5266 - val_loss: 0.6953\n",
      "Epoch 14/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5306 - loss: 0.6924 - val_accuracy: 0.5439 - val_loss: 0.6926\n",
      "Epoch 15/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5313 - loss: 0.6888 - val_accuracy: 0.5266 - val_loss: 0.6952\n",
      "Epoch 16/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5359 - loss: 0.6894 - val_accuracy: 0.5372 - val_loss: 0.6920\n",
      "Epoch 17/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5373 - loss: 0.6895 - val_accuracy: 0.5332 - val_loss: 0.6922\n",
      "Epoch 18/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5413 - loss: 0.6868 - val_accuracy: 0.5306 - val_loss: 0.6940\n",
      "Epoch 19/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5323 - loss: 0.6895 - val_accuracy: 0.5332 - val_loss: 0.6942\n",
      "Epoch 20/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5449 - loss: 0.6889 - val_accuracy: 0.5412 - val_loss: 0.6941\n",
      "Epoch 21/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5452 - loss: 0.6874 - val_accuracy: 0.5412 - val_loss: 0.6914\n",
      "Epoch 22/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5396 - loss: 0.6881 - val_accuracy: 0.5332 - val_loss: 0.6947\n",
      "Epoch 23/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5316 - loss: 0.6892 - val_accuracy: 0.5239 - val_loss: 0.6983\n",
      "Epoch 24/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5419 - loss: 0.6872 - val_accuracy: 0.5239 - val_loss: 0.7000\n",
      "Epoch 25/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5356 - loss: 0.6890 - val_accuracy: 0.5199 - val_loss: 0.6984\n",
      "Epoch 26/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5346 - loss: 0.6884 - val_accuracy: 0.4867 - val_loss: 0.7000\n",
      "Epoch 27/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5383 - loss: 0.6872 - val_accuracy: 0.5186 - val_loss: 0.6980\n",
      "Epoch 28/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5356 - loss: 0.6895 - val_accuracy: 0.5306 - val_loss: 0.6981\n",
      "Epoch 29/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5389 - loss: 0.6863 - val_accuracy: 0.5120 - val_loss: 0.7021\n",
      "Epoch 30/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5476 - loss: 0.6852 - val_accuracy: 0.5359 - val_loss: 0.6998\n",
      "Epoch 31/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5306 - loss: 0.6894 - val_accuracy: 0.5293 - val_loss: 0.6980\n",
      "Epoch 32/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5489 - loss: 0.6863 - val_accuracy: 0.5346 - val_loss: 0.6994\n",
      "Epoch 33/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5456 - loss: 0.6859 - val_accuracy: 0.5226 - val_loss: 0.6978\n",
      "Epoch 34/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5546 - loss: 0.6825 - val_accuracy: 0.5173 - val_loss: 0.7005\n",
      "Epoch 35/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5329 - loss: 0.6882 - val_accuracy: 0.5120 - val_loss: 0.7023\n",
      "Epoch 36/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5466 - loss: 0.6850 - val_accuracy: 0.4987 - val_loss: 0.7004\n",
      "Epoch 37/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5439 - loss: 0.6851 - val_accuracy: 0.5106 - val_loss: 0.7000\n",
      "Epoch 38/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5413 - loss: 0.6879 - val_accuracy: 0.5332 - val_loss: 0.6970\n",
      "Epoch 39/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5429 - loss: 0.6860 - val_accuracy: 0.5293 - val_loss: 0.7081\n",
      "Epoch 40/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5466 - loss: 0.6848 - val_accuracy: 0.5239 - val_loss: 0.6986\n",
      "Epoch 41/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5422 - loss: 0.6849 - val_accuracy: 0.5146 - val_loss: 0.6999\n",
      "Epoch 42/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5449 - loss: 0.6868 - val_accuracy: 0.5399 - val_loss: 0.6958\n",
      "Epoch 43/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5439 - loss: 0.6832 - val_accuracy: 0.5293 - val_loss: 0.6993\n",
      "Epoch 44/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5456 - loss: 0.6852 - val_accuracy: 0.5239 - val_loss: 0.7012\n",
      "Epoch 45/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5459 - loss: 0.6832 - val_accuracy: 0.4840 - val_loss: 0.7189\n",
      "Epoch 46/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5532 - loss: 0.6821 - val_accuracy: 0.5412 - val_loss: 0.7037\n",
      "Epoch 47/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5462 - loss: 0.6832 - val_accuracy: 0.4920 - val_loss: 0.7081\n",
      "Epoch 48/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5516 - loss: 0.6827 - val_accuracy: 0.5332 - val_loss: 0.7067\n",
      "Epoch 49/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5466 - loss: 0.6831 - val_accuracy: 0.5293 - val_loss: 0.7077\n",
      "Epoch 50/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5512 - loss: 0.6826 - val_accuracy: 0.5306 - val_loss: 0.6982\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "Best MLP Model Performance:\n",
      "Accuracy : 0.5309\n",
      "Precision: 0.5405\n",
      "Recall   : 0.7525\n",
      "F1-Score : 0.6291\n",
      "\n",
      "Confusion Matrix:\n",
      "[[125 318]\n",
      " [123 374]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.28      0.36       443\n",
      "           1       0.54      0.75      0.63       497\n",
      "\n",
      "    accuracy                           0.53       940\n",
      "   macro avg       0.52      0.52      0.50       940\n",
      "weighted avg       0.52      0.53      0.50       940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrain Best MLP Model and Evaluate\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the best model architecture based on tuner\n",
    "best_mlp = Sequential([\n",
    "    Dense(96, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.4),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with the best learning rate\n",
    "best_mlp.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = best_mlp.fit(X_train_scaled, y_train_res,\n",
    "                       validation_split=0.2,\n",
    "                       epochs=50,\n",
    "                       batch_size=32,\n",
    "                       verbose=1)\n",
    "\n",
    "# Predict on the original test set\n",
    "y_pred_best = (best_mlp.predict(X_test_scaled) > 0.5).astype(int)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_best)\n",
    "precision = precision_score(y_test, y_pred_best)\n",
    "recall = recall_score(y_test, y_pred_best)\n",
    "f1 = f1_score(y_test, y_pred_best)\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "print(\"Best MLP Model Performance:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-Score : {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ca78b39-08be-4bb3-a3f1-ba89500d1751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP trained successfully\n"
     ]
    }
   ],
   "source": [
    "mlp_wrapper.fit(X_train_res_scaled, y_train_res)\n",
    "# Do not put mlp_wrapper alone as the last line\n",
    "print(\"MLP trained successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f7f2c79-677a-4fa9-84af-3536e9cad550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Soft-Voting Ensemble Performance:\n",
      "Accuracy : 0.4713\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-Score : 0.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[443   0]\n",
      " [497   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      1.00      0.64       443\n",
      "           1       0.00      0.00      0.00       497\n",
      "\n",
      "    accuracy                           0.47       940\n",
      "   macro avg       0.24      0.50      0.32       940\n",
      "weighted avg       0.22      0.47      0.30       940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1️ Generate probabilities for the positive class\n",
    "rf_pred_prob = rf_model.predict_proba(X_test_scaled)[:,1]\n",
    "xgb_pred_prob = xgb_model.predict_proba(X_test_scaled)[:,1]\n",
    "mlp_pred_prob = mlp_wrapper.predict_proba(X_test_scaled)[:,1]  # MLP fitted\n",
    "\n",
    "# 2️ Manual soft voting (average probabilities)\n",
    "ensemble_prob = (rf_pred_prob + xgb_pred_prob + mlp_pred_prob) / 3\n",
    "y_pred_ensemble = (ensemble_prob >= 0.5).astype(int)\n",
    "\n",
    "# 3️ Evaluate ensemble\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "precision = precision_score(y_test, y_pred_ensemble)\n",
    "recall = recall_score(y_test, y_pred_ensemble)\n",
    "f1 = f1_score(y_test, y_pred_ensemble)\n",
    "cm = confusion_matrix(y_test, y_pred_ensemble)\n",
    "\n",
    "print(\"Manual Soft-Voting Ensemble Performance:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-Score : {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8830aead-839c-4531-b903-7fa432a502c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF prediction counts: [940]\n",
      "XGB prediction counts: [940]\n",
      "MLP prediction counts: [  6 934]\n"
     ]
    }
   ],
   "source": [
    "print(\"RF prediction counts:\", np.bincount(rf_model.predict(X_test_scaled)))\n",
    "print(\"XGB prediction counts:\", np.bincount(xgb_model.predict(X_test_scaled)))\n",
    "print(\"MLP prediction counts:\", np.bincount(mlp_wrapper.predict(X_test_scaled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "989378db-8ba6-4673-a59a-e65c9416068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_prob = (0.2*rf_pred_prob + 0.2*xgb_pred_prob + 0.6*mlp_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a54f15c7-1762-44f2-b998-34d30c2cd582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Performance with threshold = 0.3\n",
      "Accuracy : 0.5287234042553192\n",
      "Precision: 0.5287234042553192\n",
      "Recall   : 1.0\n",
      "F1-Score : 0.6917188587334725\n",
      "Confusion Matrix:\n",
      " [[  0 443]\n",
      " [  0 497]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       443\n",
      "           1       0.53      1.00      0.69       497\n",
      "\n",
      "    accuracy                           0.53       940\n",
      "   macro avg       0.26      0.50      0.35       940\n",
      "weighted avg       0.28      0.53      0.37       940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.3  # experiment with 0.3, 0.35, 0.4, etc.\n",
    "ensemble_pred = (ensemble_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"Ensemble Performance with threshold =\", threshold)\n",
    "print(\"Accuracy :\", accuracy_score(y_test, ensemble_pred))\n",
    "print(\"Precision:\", precision_score(y_test, ensemble_pred))\n",
    "print(\"Recall   :\", recall_score(y_test, ensemble_pred))\n",
    "print(\"F1-Score :\", f1_score(y_test, ensemble_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, ensemble_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, ensemble_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72aa8941-8c54-41c2-88d0-5a0055ed14df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated Ensemble Performance with threshold = 0.5\n",
      "Accuracy : 0.5287234042553192\n",
      "Precision: 0.5287234042553192\n",
      "Recall   : 1.0\n",
      "F1-Score : 0.6917188587334725\n",
      "Confusion Matrix:\n",
      " [[  0 443]\n",
      " [  0 497]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       443\n",
      "           1       0.53      1.00      0.69       497\n",
      "\n",
      "    accuracy                           0.53       940\n",
      "   macro avg       0.26      0.50      0.35       940\n",
      "weighted avg       0.28      0.53      0.37       940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MLP predictions (probability for class 1 only)\n",
    "mlp_pred_prob = mlp_wrapper.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Weighted soft-voting ensemble\n",
    "ensemble_prob_cal = (0.3 * rf_pred_prob_cal + \n",
    "                     0.3 * xgb_pred_prob_cal + \n",
    "                     0.4 * mlp_pred_prob)\n",
    "\n",
    "# Apply threshold\n",
    "threshold = 0.5\n",
    "ensemble_pred_cal = (ensemble_prob_cal >= threshold).astype(int)\n",
    "\n",
    "# Evaluate ensemble\n",
    "print(\"Calibrated Ensemble Performance with threshold =\", threshold)\n",
    "print(\"Accuracy :\", accuracy_score(y_test, ensemble_pred_cal))\n",
    "print(\"Precision:\", precision_score(y_test, ensemble_pred_cal))\n",
    "print(\"Recall   :\", recall_score(y_test, ensemble_pred_cal))\n",
    "print(\"F1-Score :\", f1_score(y_test, ensemble_pred_cal))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, ensemble_pred_cal))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, ensemble_pred_cal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30e1e66c-3830-4d2a-a712-bb229a98e2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.1\n",
      "Best F1-score: 0.6917188587334725\n",
      "Accuracy : 0.5287234042553192\n",
      "Precision: 0.5287234042553192\n",
      "Recall   : 1.0\n",
      "F1-Score : 0.6917188587334725\n",
      "Confusion Matrix:\n",
      " [[  0 443]\n",
      " [  0 497]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       443\n",
      "           1       0.53      1.00      0.69       497\n",
      "\n",
      "    accuracy                           0.53       940\n",
      "   macro avg       0.26      0.50      0.35       940\n",
      "weighted avg       0.28      0.53      0.37       940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "thresholds = np.arange(0.1, 0.91, 0.05)  # test thresholds from 0.1 to 0.9\n",
    "best_thresh, best_f1 = 0.5, 0\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (ensemble_prob_cal >= t).astype(int)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thresh = f1, t\n",
    "\n",
    "print(\"Best threshold:\", best_thresh)\n",
    "print(\"Best F1-score:\", best_f1)\n",
    "\n",
    "# Recalculate metrics with best threshold\n",
    "final_preds = (ensemble_prob_cal >= best_thresh).astype(int)\n",
    "print(\"Accuracy :\", accuracy_score(y_test, final_preds))\n",
    "print(\"Precision:\", precision_score(y_test, final_preds))\n",
    "print(\"Recall   :\", recall_score(y_test, final_preds))\n",
    "print(\"F1-Score :\", f1_score(y_test, final_preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, final_preds))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, final_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5b2d92-5a30-49a6-ba4a-1c81f4ba5c0c",
   "metadata": {},
   "source": [
    "# Author of project: Akinmade Faruq\n",
    "# Contact informations: www.linkedin.com/in/faruqakinmade\n",
    "# Email: Fharuk147@gmail.com\n",
    "# X website: https://x.com/EngrrrAkinmade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9581d792-a4d6-4852-9c02-75946ee69531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
